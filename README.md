# reading_list

> * Python is highly required. C++ will help you.
> * Pytorch (or Tensorflow) is highly required.
> * People who want to do FPGA stuff is not required for python or pytorch.
> * **Person who misses two reports will leave this group automatically.**:bangbang: If you cannot find your name blow, please add it by yourself.
> * Please contact with @wetian to join our mail list. 

> routine of weekly meeting:
> 1. submit report before meeting.
> 2. each member updates the research activities conducted in the past week (2-3 minutes). 
> 3. reading group: story one person will be the presenter. at this stage, your responsibility is to present published papers of a theme (a story summarized from 3-5 relevant papers). 
> 4. the presentation is for half hour, slides will help you a lot.
Presenter are supposed to upload their slides to github.
> 5. the information of report should be uploaded [here](./report_record.md)
  

### Week 1:

#### Plan: 

1. Reading codes of [XNOR-Net codes](https://github.com/jiecaoyu/XNOR-Net-PyTorch)
2. Reading paper of [XNOR-Net paper](https://github.com/allenai/XNOR-Net)
3. Understand meaning of xnor operation and concept of quantization networks.

#### Extensive Reading (Compulsory):
1. [Overcoming Challenges in Fixed Point Training of Deep Convolutional Networks](./Krishnamoorthi%20-%202018%20-%20Quantizing%20deep%20convolutional%20networks%20for%20efficient%20inference%20A%20whitepaper.pdf) [Qualcomm Research]
2. [Quantizing deep convolutional networks for efficient inference: A whitepaper](./Lin%2C%20Talathi%20-%202016%20-%20Overcoming%20Challenges%20in%20Fixed%20Point%20Training%20of%20Deep%20Convolutional%20Networks.pdf) [Google]

#### Submit:
|#|Name|File|
|---|---|----
|:heavy_check_mark:|Weitian|[example](./Lin%2C%20Talathi%20-%202016%20-%20Overcoming%20Challenges%20in%20Fixed%20Point%20Training%20of%20Deep%20Convolutional%20Networks.pdf)|
|:heavy_check_mark:|Qian|[example](./Lin%2C%20Talathi%20-%202016%20-%20Overcoming%20Challenges%20in%20Fixed%20Point%20Training%20of%20Deep%20Convolutional%20Networks.pdf)|
|:heavy_check_mark:|Yuchen|[example](./Lin%2C%20Talathi%20-%202016%20-%20Overcoming%20Challenges%20in%20Fixed%20Point%20Training%20of%20Deep%20Convolutional%20Networks.pdf)|

-------------------

### Week 2:

#### Plan: 

1. Understand codes of XNOR-Net/util.py [codes](https://github.com/jiecaoyu/XNOR-Net-PyTorch/blob/master/CIFAR_10/util.py)
2. Backpropagation of neural network [link](http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm)
3. Backpropagation of quantization neural network 
4. Tensorflow version of xnor

#### Extensive Reading (Compulsory):
1.  [Review of quantization networks](https://www.jiqizhixin.com/articles/2018-06-01-11) [Xijiao University]
2.  DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Megvii, Face ++]

#### Submit:
|#|Name|File|
|---|---|----
|:heavy_check_mark:|Weitian|[week2](https://github.com/XinDongol/reading_list/blob/master/Weitian%20Li/Week%202%20Report.pdf)|
|:heavy_check_mark:|Qian|[week1&2](https://github.com/XinDongol/reading_list/blob/master/Qian%20Jiang/week1%20%262.pdf)|
|:heavy_check_mark:|Yuchen|[week2](https://github.com/XinDongol/reading_list/blob/master/Yuchen%20Cai/Report/W2%20Report.pdf)
|:heavy_check_mark:|Kexin|[week2](https://github.com/XinDongol/reading_list/blob/master/Kexin%20Fan/Week%202.pdf)
|:heavy_check_mark:|Yufei|[week2](https://github.com/XinDongol/reading_list/blob/master/Yufei%20Wang/week%202.md)

-------------------

### Week 3:
1. Another version of [XNOR-Net](https://github.com/cooooorn/Pytorch-XNOR-Net) (With CUDA code in C++)
2. Backpropagation of quantization neural network 
3. Report learning curve of CIFAR-10 of XNOR-Net

#### Extensive Reading (Compulsory):
1. Finish reading of previous articles.
2. Deep Learning with Limited Numerical Precision [2015 ICML]


#### Submit:
|#|Name|File|
|---|---|----
|:heavy_check_mark:|Weitian|[week3](https://github.com/XinDongol/reading_list/blob/master/Weitian%20Li/Week%203%20Report.pdf)|
|:heavy_check_mark:|Qian|[week3](https://github.com/XinDongol/reading_list/blob/master/Qian%20Jiang/Week%203.pdf)
|:heavy_check_mark:|Yuchen|[week3](https://github.com/XinDongol/reading_list/blob/master/Yuchen%20Cai/Report/Week3.pdf)
|:heavy_check_mark:|Kexin|[week3](https://github.com/XinDongol/reading_list/blob/master/Kexin%20Fan/Week%203.pdf)
|:x:|Yudian|
|:heavy_check_mark:|Yufei|[week3](https://github.com/XinDongol/reading_list/blob/master/Yufei%20Wang/week%203.md)



-------------------

### Week 4
1. Summarized optimization problems and methods for quantization neural networks 
2. Read code of [HWGQ](https://github.com/zhaoweicai/hwgq)


#### Extensive Reading (Compulsory):
1. Training Quantized Nets: A Deeper Understanding
2. Deep Learning with Limited Numerical Precision
3. Deep Learning with Low Precision by Half-wave Gaussian Quantization
4. Training and Inference with Integers in Deep Neural Networks


#### Submit:
|#|Name|File|
|---|---|----
|:heavy_check_mark:|Weitian|[week4](https://github.com/XinDongol/reading_list/blob/master/Weitian%20Li/Week%204%20Report.pdf)
|:heavy_check_mark:|Qian|[week4](https://github.com/XinDongol/reading_list/blob/master/Qian%20Jiang/Week%204.pdf)
|:clock3:|Yuchen|
|:heavy_check_mark:|Kexin|[week4](https://github.com/XinDongol/reading_list/blob/master/Kexin%20Fan/Week%204.pdf)
| :x:|Yudian|
|:heavy_check_mark:|Yufei|[week4](https://github.com/XinDongol/reading_list/blob/master/Yufei%20Wang/week%204.md)

-------------------


### Week 5
1. Reproduce [HWGQ](https://github.com/zhaoweicai/hwgq) with TensorFlow (If you want to join in one paper now, you should do this.) [Here](https://gist.github.com/XinDongol/14b5e6273fd8af5a3b7683947038a2eb) is my implement.
2. Write HWGQ layer in HWGQ with Cuda.

#### Extensive Reading (Compulsory):
1. Training Quantized Nets: A Deeper Understanding
2. Deep Learning with Limited Numerical Precision
3. Deep Learning with Low Precision by Half-wave Gaussian Quantization
4. Training and Inference with Integers in Deep Neural Networks


#### Submit:
|#|Name|File|
|---|---|----
|:heavy_check_mark:|Weitian|[week5](https://github.com/XinDongol/reading_list/blob/master/Weitian%20Li/Week%205%20Report.pdf)
|:heavy_check_mark:|Qian|[week5](https://github.com/XinDongol/reading_list/blob/master/Qian%20Jiang/Week%205.pdf)
|:clock3:|Yuchen|
|:clock3:|Kexin|
|:clock3:|Yudian|

-------------------


### Week 6
1. Understand [Batch Normalization](https://arxiv.org/abs/1502.03167).
2. How to do batch normalization in quantization neural network. (Please refer to paper in extensive reading)

#### Extensive Reading (Compulsory):
1. [Quantizing deep convolutional networks for efficient inference: A whitepaper](./Lin%2C%20Talathi%20-%202016%20-%20Overcoming%20Challenges%20in%20Fixed%20Point%20Training%20of%20Deep%20Convolutional%20Networks.pdf) [Google]



#### Submit:
|#|Name|File|
|---|---|----
|:clock3:|Weitian|
|:clock3:|Qian|
|:clock3:|Yuchen|
|:clock3:|Kexin|
|:clock3:|Yudian|







<!---
### Week n
1. Understand *SignSGD* and *Binaryrelax* 

#### Extensive Reading (Compulsory):
1. signSGD: compressed optimisation for non-convex problems
2. BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights

#### Submit:
|#|Name|File|
|---|---|----
|:clock3:|Weitian|
|:clock3:|Qian|
|:clock3:|Yuchen|
|:clock3:|Kexin|

-->

